{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing notebook\n"
     ]
    }
   ],
   "source": [
    "print(\"testing notebook\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><h1>Monday April 8th</h1></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3>Tutorials on YouTube</h3>\n",
    "\n",
    "Follow these tutorials to enhance your skills:\n",
    "- **Working with Sequence Data**: Learn the basics of handling genomic sequences.\n",
    "- **Searching for Information & FastQ Files**: Discover how to find relevant scientific data and work with FastQ files.\n",
    "\n",
    "## <h3>Useful Links</h3>\n",
    "- [Tutorial on Sequence Data](https://www.youtube.com/watch?v=dYvgbgEjwLs)\n",
    "- [Tutorial on Searching Information and FastQ Files](https://www.youtube.com/watch?v=2lH4uxHCeJA&list=PLNteKiuWD0-L6zFGNHNSjYYJA3xljeu2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastq files\n",
    "- most of the time sequencing data comes back as a fastq file\n",
    "- First line\n",
    "    - ID is indicated by an '@' symbol\n",
    "        - encodes spot on the plate where the cluster of fragments were\n",
    "- Second line\n",
    "    - DNA sequence\n",
    "- Third line \n",
    "    - Record ID\n",
    "        - starts with '+' sign\n",
    "- Fourth line\n",
    "    - quality score\n",
    "        - encodes information about the probability of an error of that read \n",
    "        - each character refers to a quality score\n",
    "    -  Phred scores\n",
    "        - tells how confident the machine was to identify the correct base\n",
    "            - higher the PHRED score = highly probability that it is correct. \n",
    "        - each score is associated with a character\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes!\n",
      "The DNA sequence has a GC content of 51.724137931034484 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.gcContent(DNA)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining DNA sequence Content with basic python \n",
    "\n",
    "DNA = 'GCTGACTGATCGATGCATGCTAGCTAGCT'\n",
    "\n",
    "#HOW DO YOU FIND IF THE SEQUENCE HAS A PARTICULAR SUBSEQUENCE?\n",
    "\n",
    "'CG' in DNA\n",
    "\n",
    "#Answer will be true\n",
    "\n",
    "if 'CG' in DNA:print('yes!')\n",
    "    #Reminder that if statements require a colon\n",
    "\n",
    "\n",
    "DNA.count('AT')\n",
    "\n",
    "#AT is 5\n",
    "\n",
    "#Computing GC content with python\n",
    "\n",
    "def gcContent(DNA):\n",
    "    totalC = DNA.count('C')\n",
    "    totalG = DNA.count('G')\n",
    "    gcContent = 100*(totalC + totalG)/len(DNA)\n",
    "    return gcContent\n",
    "\n",
    "print(\"The DNA sequence has a GC content of\", gcContent(DNA), \"%\")\n",
    "       \n",
    "gcContent      \n",
    "\n",
    "#answer is 51.724137931034484 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matches_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# re stands for Regular Expression\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Print the number of 'ATG' occurrences\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mATG\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m occurrences is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(matches_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfinditer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mATG\u001b[39m\u001b[38;5;124m'\u001b[39m, DNA)\n\u001b[1;32m     12\u001b[0m matches_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(matches) \u001b[38;5;66;03m# Convert iterator to a list to check its length\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matches_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Loop through a DNA sequence and find a pattern\n",
    "\n",
    "DNA = 'GCTGACTGATCGATGCATGCTAGCTAGATGCTATG'\n",
    "\n",
    "import re\n",
    "# re stands for Regular Expression\n",
    "\n",
    "# Print the number of 'ATG' occurrences\n",
    "print(f\"The number of 'ATG' occurrences is: {len(matches_list)}\")\n",
    "\n",
    "matches = re.finditer('ATG', DNA)\n",
    "matches_list = list(matches) # Convert iterator to a list to check its length\n",
    "if matches_list: \n",
    "    for match in matches_list:\n",
    "        print(f\"An 'ATG' match is located between {match.start()} and {match.end()}\")\n",
    "else:\n",
    "    print(\"No 'ATG' matches found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><h1>Wednesday April 10th</h1></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A beginner's bioinformatics guide for single-cell RNAseq data analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Reverse compliment of a sequence](https://www.youtube.com/watch?v=t9C1g6ld-ng&list=PLNteKiuWD0-L6zFGNHSNjYYJA3xljIeu2&index=11)\n",
    "- [Introduction to fastq files](https://www.youtube.com/watch?v=9bN4rWPecmw&t=53s)\n",
    "- [fasta vs fastq](https://www.youtube.com/watch?v=xYEre9DtIqA&t=1s)\n",
    "- [Galaxy training](https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101/tutorial.html)\n",
    "\n",
    "\n",
    "Sites to investigate \n",
    "- [parsing Fastq files with Biopython SeqIO moedule](https://www.youtube.com/watch?v=mDEwKw0w1Qk)\n",
    "- [intro to Biopython SeqIO module: Reading Fasta files](https://www.youtube.com/watch?v=Y9POfa_PH-M)\n",
    "- [setup RNA-seq pipeline from scratch: fastq (reads) to counts](https://www.youtube.com/watch?v=lG11JjovJHE&t=1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the reverse compliment of a sequence\n",
    "\n",
    "def complement(DNA):\n",
    "    comp = {'A': 'T', 'C':'G', 'G':'C', 'T':'A'}\n",
    "    compDNA = ''\n",
    "    for b in DNA:\n",
    "        compDNA += comp[b]\n",
    "    return compDNA\n",
    "def reverse (DNA):\n",
    "    return DNA[::-1]\n",
    "\n",
    "def reverse_complement(DNA):\n",
    "    compDNA = complement(DNA)\n",
    "    compDNA = complement(DNA)\n",
    "    revCompDNA = reverse(compDNA)\n",
    "    return revCompDNA \n",
    "\n",
    "DNA = input(\"Please enter a DNA sequence.\")\n",
    "print(\"you have entered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasta files\n",
    "- used to store sequenes (either amino acid or nucleic acid)\n",
    "    - if its a nuclear acid sequence the file could be .fna\n",
    "    - if its a amino acid sequence the file could be .faa\n",
    "- each amino acid is represented by one letter code\n",
    "- divided into parts\n",
    "    - header \n",
    "        - starts with a '>' symbol\n",
    "    - sequence\n",
    "        - can have multiple sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intro to Galaxy</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:#7393B3;\">Familiarize yourself with the basics of Galaxy</p></h3>  \n",
    "\n",
    "- Learn how to obtain data from external sources\n",
    "- Learn how to run tools\n",
    "- Learn how histories work\n",
    "- Learn how to create a workflow\n",
    "- Learn how to share your work\n",
    "\n",
    "\n",
    "<h3><p style=\"color:#7393B3;\">Question</p></h3>\n",
    "\n",
    "Which coding exon has the highest number of single nucleotide polymorphisms (SNPs) on human chromosome 22?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p style=\"color: #7393B3;\">Upload Data and Start Analysis</p></h3>\n",
    "<p style=\"color:;\">Upload the data and click start to create new datasets.</p>\n",
    "<p style=\"color:;\">You can rename the datasets afterwards.</p>\n",
    "\n",
    "<h3><p style=\"color: #7393B3;\">Find Intersections in BED (Side Panel)</p></h3>\n",
    "<p style=\";\">Use the BED tools \"intersect intervals\" function.</p>\n",
    "<p><p style=\"color: pink;\">This will intersect dataset A with dataset B.</p>\n",
    "\n",
    "<h3><p style=\"color: #7393B3;\">Analysis Steps</h3>\n",
    "<h4>1. Intersect Exons with SNPs</h4>\n",
    "<p style=\"color: pink;\">This step finds overlapping regions between exons and SNPs.</p>\n",
    "\n",
    "<h4>2. Strandedness-Based Calculations </h4>\n",
    "<p style=\"color: teal;\">Analyze overlaps based on the strand orientation (positive or negative) of the features (if applicable).</p>\n",
    "  <p style=\"color: pink;\">This step considers overlaps on either strand.</p>\n",
    "\n",
    "\n",
    "  <h4>3. What should be written to the output file?</h4>\n",
    "   <p style=\"color: teal;\">\"Write the original entry in b......\"</p>\n",
    "\n",
    "  <h4>4. Click Run Tool</h4> \n",
    "   <p style=\"color: pink;\">This will create a new dataset\n",
    "   <br>\n",
    "   This will return the exon and snp information\n",
    "   <br>\n",
    "   Easy to see how many snps per exons it has\n",
    "   <br>\n",
    "   Count lines with unique line exon Ids \n",
    "   <br>\n",
    "   \n",
    "   </p>  \n",
    "\n",
    "\n",
    "\n",
    "<h3><p style=\"color: #7393B3;\"> Find Datamash (Side Panel)</p></h3>\n",
    "\n",
    "<h4>1. Group by feilds</h4>\n",
    "<p style=\"color: teal;\">\"4\"</p>\n",
    "\n",
    "  <p style=\"color: pink;\">what is the column number you are interested in? In this case it would be column 4 because this is where the exon ID is</p>\n",
    "\n",
    "  <h4>2. Operation to perform on each group Type:</h4>\n",
    "<p style=\"color: teal;\">\"count unique values\"</p>\n",
    "<h4>3. On column:</h4>\n",
    "<p style=\"color: teal;\">\"10\"</p>\n",
    "\n",
    "  <p style=\"color: pink;\">This is where the SNP IDs are</p>\n",
    "\n",
    "<h4>4. Run tool:</h4>\n",
    "\n",
    "<p style=\"color: pink;\">This will return a new dataset that contains exon ID and the count of unique snps within the exon. View this new data sheet</p>\n",
    "\n",
    "\n",
    "\n",
    "<h3><p style=\"color: #7393B3;\"> Find Text Manipulation (Side Panel)</p></h3>\n",
    "\n",
    "<h4>1. Sort:</h4>\n",
    "\n",
    "<h4>2. On column:</h4>\n",
    "<p style=\"color: teal;\">\"Column 2\"</p>\n",
    "\n",
    "<h4>3. With flavor:</h4>\n",
    "<p style=\"color: teal;\">\"Desending order\"</p>\n",
    "\n",
    "\n",
    "<p style=\"color: pink;\">This will return a new dataset that contains exon ID and the count of unique snps within the exon in descending order. View this new data sheet. In this dataset the most number of SNPs is 27 on ENST00000253255.7_cds_0_0_chr22_46256561_r </p>\n",
    "\n",
    "\n",
    "<h3><p style=\"color: #7393B3;\"> Find Text Manipulation (Side Panel)</p></h3>\n",
    "\n",
    "<h4>1. Select first:</h4>\n",
    "<p style=\"color: teal;\">\"5\"</p>\n",
    "\n",
    "\n",
    "<p style=\"color: pink;\">This will return a new dataset that contains exon ID with the highest number of SNPS in desending order. Next step is to visualze this in a genome browser. For this we need the start and end coordinates. Which is located in the original files </p>\n",
    "\n",
    "\n",
    "<h3><p style=\"color: #7393B3;\"> Find Join, Subtract, and Group (Side Panel)</p></h3>\n",
    "\n",
    "<h4>1. Compare two datasets:</h4>\n",
    "<p style=\"color: teal;\">\"Compare exons (insert file)\"</p>\n",
    "<h4>2. Using column</h4>\n",
    "<p style=\"color: teal;\">\"column 4\"</p>\n",
    "<h4>3. Against</h4>\n",
    "<p style=\"color: teal;\">\"Select first on data 5 (insert file)\"</p>\n",
    "<p style=\"color: teal;\">\"column 1\"</p>\n",
    "<h4>4. To find </h4>\n",
    "<p style=\"color: teal;\">\"Matching rows of 1st dataset\"</p>\n",
    "<h4>5. Run Tool </h4>\n",
    "\n",
    "<p style=\"color: pink;\">This will return a new dataset that contains exon ID with the highest number of SNPS in desending order. AND the start and end sites. \n",
    "<br> Next step is to identify which version of the genome we want to render the data \n",
    "<br>Tell the data that it is derived from human\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "<h3><p style=\"color: #7393B3;\"> Find database (History Panel under title)</p></h3>\n",
    "\n",
    "<h4>1. Click on the question mark </h4>\n",
    "<h4>2. Scroll to \"Database/Build\"</h4>\n",
    "<p style=\"color: teal;\">\"Select human\"</p>\n",
    "\n",
    "\n",
    "<p style=\"color: pink;\">This will now change the \"?\" to \"hg38\"\n",
    "\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "This should display something called \n",
    "'display at UCSC main' where main is a hyperlink to a genome browser to see exon of interest based on the coordinates you have. This should return information about the exon. \n",
    "\n",
    "\n",
    "This is not working out, moving on.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"color: pink;\">\n",
    "What you have created on the history panel is essential a workflow, and you want to do the same analysis on a different set of data. \n",
    "<br>You can extract workflow of this history by clicking the gear icon</p>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><h1>Wednesday April 11th</h1></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parsing Fastq files with Biopython SeqIO moedule</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to process a fastq file using a Biopython SeqIO module\n",
    "\n",
    "you can use this to identify peaks in the genome where heatshock factor binds and motif finding and what it looks like.\n",
    "\n",
    "**there are some other tools like skewer or fastq trimmer that you will most likely use instead. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Setup RNA-seq pipeline from scratch: fastq (reads) to counts </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"workflow.png\" alt=\"Example Image\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "you can identify transcripts by mapping reads onto the genome\n",
    "\n",
    "well annotated genomes like human genome we can base RNA seq analysis based on existing annotioation and references available \n",
    "\n",
    "used rna seq data allele specific expression and disease assocated SNPs and gene fushions in cancer\n",
    "\n",
    "depending on the research goal the analysis can be selected and appropriately applied \n",
    "\n",
    "Schematic to process RNA seq reads\n",
    "\n",
    "First step \n",
    "- perform quality control of reads that are in fastq files\n",
    "    - use tools like \n",
    "        - fastq screen\n",
    "         - fastq c \n",
    "\n",
    "    - goal is to remove bases that are poor quality and adaptor sequences \n",
    "        - adapter sequences are ligated to the ends of DNA fragment to faciliate their amplification and sequencings and are needed for processing, but these are not a part of the original DNA/ RNA sample and contain no relevant biological information\n",
    "\n",
    "step 2\n",
    "- trimming step\n",
    "- tool\n",
    "    - timmermatic\n",
    "    - cut adapt \n",
    "\n",
    "step 3\n",
    "- map the reads to a genome or a transcriptome\n",
    "\n",
    "- mapping to a genome\n",
    "    - need to use splicedware aligners\n",
    "        - RNA seq reads are matured from mature mRNA which includes exons only and no introns so if you try to align this to the reference sequence which is not splicedware, it would try to map these rna seq reads to references containing intron wihch are not preseent in your RNA seq reads. thus, it will not be aligned. you need an aligner that will identify the downstream exons \n",
    "        - software\n",
    "            - star\n",
    "            - tophat 2\n",
    "            - high cycle\n",
    "\n",
    "- mapping to a transcriptome \n",
    "    - this tool does not report alignments, but associate a read to a given transcript for quantification. the output from these are \"Counts\"\n",
    "    -splicedunaware aligner\n",
    "        - software \n",
    "            - bowtie 2\n",
    "            - vwa\n",
    "    - quasi mapper\n",
    "        - software\n",
    "            - salmon\n",
    "            - kallistro\n",
    "\n",
    "\n",
    "\n",
    "Step 4\n",
    "\n",
    "Spliced aware and unaware aligners need to be further processed and quantifid to generate counts \n",
    "- softwards\n",
    "    - RSEM\n",
    "    - eXpress\n",
    "\n",
    "\n",
    "Step 5\n",
    "\n",
    "Once you have a count file, you can do a downstream analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"workflow 2.png\" alt=\"Example Image\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><h1>Monday April 22-26</h1></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sequence Data Processing\n",
    "FASTQ files obtained from sequencing facility are processed using <a href=\"http://www.usegalaxy.org\">Galaxy</a> and in-house python scripts to ultimately obtain tabular files that map Tn sequencing reads to individual sequence sites. \n",
    "\n",
    "### <span style=\"color: #40E0D0;\">FASTQ: The raw sequencing reads</span>\n",
    "1. To download FASTQs from the <a href=\"https://www.ncbi.nlm.nih.gov/sra\">Sequence Reads Archive </b></a> add Accession ID: <b>PRJNA558044</b>\n",
    "    - This refers to the project ID not the individual sing sequencing ID. \n",
    "2. Select <b>\"Fastq\"</b>\n",
    "    - SRA section shows how many experiments (51)\n",
    "3. Select <b>\"Send results to Run selector\"</b>\n",
    "    - <img src=\"Fastq file from SRA.png\" alt=\"Example Image\" style=\"width: 200px;\"/>\n",
    "\n",
    "4. Select all and compute into \"Galaxy\"    \n",
    "\n",
    "4. Download the files from <a href = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6934316/\">paper's github</a>  (19 files)\n",
    "    - __Galaxy-Workflow-TnSeq.ga__ - The workflow used in Galaxy to process the FASTQ files into SAM files. \n",
    "    - __Galaxy-SampleBarcodes.txt__ - An input for the Galaxy workflow that contains the six sample barcodes used to split the raw FASTQ from the pooled sample into the FASTQs for individual samples. \n",
    "        - __Galaxy-TransposonBarcodes.txt__ - An input for the Galaxy workflow that contains the six transposon barcodes used to split each individual sample FASTQ by transposon construct. \n",
    "    - __Filter1.txt__ - An input for the Galaxy workflow used to get rid of unmatched reads. \n",
    "    - __Filter2.txt__ - An input for the Galaxy workflow used to get rid of unmatched reads.\n",
    "    - __Filter3.txt__ - An input for the Galaxy workflow used to get rid of unmatched reads.\n",
    "    - __*.fasta__ - A chromosome nucleotide fasta for the genome you are mapping to. See paper methods for the NCBI accession numbers for the strains used.  \n",
    "    - __sam_to_tabular.py__ - A python script that converts all SAM files in the directory to tab-delimited hop count files. \n",
    "\n",
    "#### <span style=\"color: #40E0D0;\">Process</span>\n",
    "1. Sign into <a href=\"http://www.usegalaxy.org\">Galaxy</a> and upload FASTQ, workflow, barcodes, filters, and fasta(s).\n",
    "   - There is an option to get SRA files within galaxy\n",
    "2. Search for accesson ID (PRJNA55804)in Sequence Reads Archive\n",
    "3. Under \"File Type\" click Fastq\n",
    "4. Next to \"View results as an expanded interactive table using the RunSelector\" \n",
    "5. Select all and under computing select \"Galaxy\"\n",
    "6. <img src=\"Fastq file from SRA.png\" alt=\"Example Image\" style=\"width: 100px;\"/> \n",
    "\n",
    "\n",
    "7. Based on the paper's method section <a ref =\"https://www.ncbi.nlm.nih.gov/genome\"> find fasta files </a> using sequence Accession IDs:\n",
    "    - You can use each accession ID for the specific strains mentioned \n",
    "        1.  <a ref = \"https://www.ncbi.nlm.nih.gov/nuccore/NC_007795.1/\"> NC_007795.1 (parent strain) </a>\n",
    "        2. NC_010079.1 (USA300-TCH1516)\n",
    "        3. NC_002953.3 (MSSA476)\n",
    "        4. NC_003929.1 (MW2) // new : NC_003923.1\n",
    "        5. NC_002952.2 (MRSA252)\n",
    "\n",
    "8. Enter each accession ID one at a time in the search bar.\n",
    "\n",
    "    - After searching, click on the link to the specific genome.\n",
    "    - On the genome summary page, look for a link to download the genome in FASTA format. \n",
    "    - This is typically under a section labeled \"FTP\" or directly available as a \"Download\" button with options where you can select \"FASTA.\"\n",
    "\n",
    "9. Verify and Download:\n",
    "\n",
    "    Ensure you're downloading the right files by checking the strain and version number.\n",
    "    Download the file to your local machine.\n",
    "\n",
    "10. Upload fasta files to Galaxy:\n",
    "  \n",
    "11. Go to tab workflow, Upload Galaxy-workflow-tnSeq.ga from Walker github Repo files\n",
    "12. Click on Run...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Upload sequencing data : <b>SRR9899156</b>\n",
    "2. Sample Barcode:  <b>Galaxy-SampleBarcodes.txt</b>\n",
    "3. Tn Barcodes: <b>Galaxy-TransposonBarcode.txt</b>\n",
    "4. Filter 1: <b>Filter 1</b>\n",
    "5. Filter 2: <b>Filter 2</b>\n",
    "6. Filter 3: <b>Filter 3</b>\n",
    "7. FASTA file for genome\n",
    "    - example: <b>NC_007795.1.fasta</b>\n",
    "8. Follow the prompts to select the appropriate inputs. \n",
    "If the FASTQ file has samples from multiple genomes, run the workflow once with each fasta file and only save the appropriate SAM files.\n",
    "9. Download the resulting SAM files into a directory containing the sam_to_tabular.py script. \n",
    "10. Sometimes in life, the files don't provide the information you are looking for.\n",
    "    - download the fasta files <a ref = \"https://www.ncbi.nlm.nih.gov/nuccore/NC_007795.1/#feature_NC_007795.1\"> again </a> \n",
    "\n",
    "    - under the send to \n",
    "    - chose \"coding sequence\"\n",
    "    - chose \"FASTA nucleotide\"\n",
    "    - create file\n",
    "11. Download it to one folder\n",
    "12. Upload the multi-fasta files into galaxy. Tell it to upload it in the fasta format\n",
    "13. Run the workflow again to get SAM Files\n",
    "\n",
    "\n",
    "\n",
    " ## start here\n",
    "\n",
    "### <span style=\"color: #40E0D0;\">SAM FILES to Tabular</span>\n",
    "1. Run the sam_to_tabular.py script `python sam_to_tabular.py`\n",
    "    1. The Script is in the <a ref=\"https://github.com/SuzanneWalkerLab/5SATnSeq/blob/master/sam_to_tabular.py\">Walker github repo</a>\n",
    "    2. I downloaded this file into my Repo and synced the changes\n",
    "        -  You need to ensure that this script is on your Galaxy server or your local environment where you're working with the Galaxy interface.\n",
    "        - You usually would use a command like python sam_to_tabular.py in a command line interface \n",
    "\n",
    "\n",
    "        - <span style=\"color: #FFC4CA;\">Why Convert from SAM to Tabular?</span>\n",
    "            - SAM files: good for storing alignment data, but clunky for analysis.\n",
    "            - Tabular format: Easier to read, manipulate (filtering, sorting), and visualize (charts, graphs).\n",
    "            - This Python script is presumably designed to read SAM files and convert them into a tabular format (.tabular).\n",
    "            - ou need to ensure that this script is on your Galaxy server or your local environment where you're working with the Galaxy interface\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: reconstruct the hopcount tool we use in Tufts Galaxy (galaxy.med.tufts.edu) that is not present on usegalaxy.org.  \n",
    "\n",
    "import glob \n",
    "\n",
    "sams = glob.glob('/Users/tailob/Desktop/jax/repositories/tnseq-meta-integration/Files/multi_fasta_SAM/*/*.sam')\n",
    "\n",
    "for s in sams:\n",
    "\tdatadict = {}\n",
    "\tfor line in open(s,'r').readlines():\n",
    "\t\tif not line.startswith('@') and not line.startswith('/'):\n",
    "\t\t\tinfo = line.split('\\t')\n",
    "\t\t\tflag = info[1]; ref = info[2]; site = int(info[3]); seq = info[9]\n",
    "\t\t\tif flag == '0': \n",
    "\t\t\t\tif (ref,site) in datadict: \n",
    "\t\t\t\t\tdatadict[(ref,site)][0]+=1\n",
    "\t\t\t\telse: datadict[(ref,site)]=[1,0]\n",
    "\t\t\telif flag == '16':\n",
    "\t\t\t\tif (ref,site+15) in datadict: \n",
    "\t\t\t\t\tdatadict[(ref,site+15)][1]+=1\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tdatadict[(ref,site+15)]=[0,1]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\toutput = open(s[:-3]+'tabular','w')\n",
    "\toutput.write('Reference\\tPosition\\tLocus\\tGene\\tPlusCount\\tMinusCount\\tTotalCount\\tProduct\\tProteinID\\tNote\\tSequence\\n')\n",
    "\tdatakeys = sorted(datadict.keys(),key=lambda x: x[1])\n",
    "\tfor k in datakeys: \n",
    "\t\toutput.write(k[0]+'\\t'+str(k[1])+'\\t\\t\\t')\n",
    "\t\toutput.write('%i\\t%i\\t%i\\t\\t\\t\\t\\n' %(datadict[k][0],datadict[k][1],datadict[k][0]+datadict[k][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run sam_to_tabular(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gffs = glob.glob(\"./Files/multi_fasta_SAM/*.sam\")\n",
    "print((gffs))\n",
    "for g in gffs: \n",
    "    print(g)\n",
    "    make_prot_table(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# GOAL: reconstruct the hopcount tool we use in Tufts Galaxy (galaxy.med.tufts.edu) that is not present on usegalaxy.org.  \n",
    "\n",
    "import glob \n",
    "\n",
    "sams = glob.glob('~/Users/tailob/Dropbox/Breeya_rotation/Files/multi_fasta_SAM/3389_SAM')\n",
    "print(len(sams))\n",
    "\n",
    "\n",
    "for s in sams:\n",
    "\tdatadict = {}\n",
    "\tfor line in open(s,'r').readlines():\n",
    "\t\tif not line.startswith('@') and not line.startswith('/'):\n",
    "\t\t\tinfo = line.split('\\t')\n",
    "\t\t\tflag = info[1]; ref = info[2]; site = int(info[3]); seq = info[9]\n",
    "\t\t\tif flag == '0': \n",
    "\t\t\t\tif (ref,site) in datadict: \n",
    "\t\t\t\t\tdatadict[(ref,site)][0]+=1\n",
    "\t\t\t\telse: datadict[(ref,site)]=[1,0]\n",
    "\t\t\telif flag == '16':\n",
    "\t\t\t\tif (ref,site+15) in datadict: \n",
    "\t\t\t\t\tdatadict[(ref,site+15)][1]+=1\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tdatadict[(ref,site+15)]=[0,1]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\toutput = open(s[:-3]+'tabular','w')\n",
    "\toutput.write('Reference\\tPosition\\tLocus\\tGene\\tPlusCount\\tMinusCount\\tTotalCount\\tProduct\\tProteinID\\tNote\\tSequence\\n')\n",
    "\tdatakeys = datadict.keys()\n",
    "\t#datakeys.sort(key=lambda x: x[1])\n",
    "\tfor k in datakeys: \n",
    "\t\toutput.write(k[0]+'\\t'+str(k[1])+'\\t\\t\\t')\n",
    "\t\toutput.write('%i\\t%i\\t%i\\t\\t\\t\\t\\n' %(datadict[k][0],datadict[k][1],datadict[k][0]+datadict[k][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Files/multi_fasta_SAM/4483_SAM/Sample2_blunt.sam', './Files/multi_fasta_SAM/4483_SAM/Sample6_dual.sam', './Files/multi_fasta_SAM/4483_SAM/Sample1_erm.sam', './Files/multi_fasta_SAM/4483_SAM/Sample3_tuf.sam', './Files/multi_fasta_SAM/4483_SAM/Sample6_cap.sam', './Files/multi_fasta_SAM/4483_SAM/Sample1_pen.sam', './Files/multi_fasta_SAM/4483_SAM/Sample2_tuf.sam', './Files/multi_fasta_SAM/4483_SAM/Sample4_blunt.sam', './Files/multi_fasta_SAM/4483_SAM/Sample3_blunt.sam', './Files/multi_fasta_SAM/4483_SAM/Sample2_erm.sam', './Files/multi_fasta_SAM/4483_SAM/Sample3_pen.sam', './Files/multi_fasta_SAM/4483_SAM/Sample4_cap.sam', './Files/multi_fasta_SAM/4483_SAM/Sample5_blunt.sam', './Files/multi_fasta_SAM/4483_SAM/Sample1_dual.sam', './Files/multi_fasta_SAM/4483_SAM/Sample2_pen.sam', './Files/multi_fasta_SAM/4483_SAM/Sample3_erm.sam', './Files/multi_fasta_SAM/4483_SAM/Sample5_cap.sam', './Files/multi_fasta_SAM/4483_SAM/Sample1_tuf.sam', './Files/multi_fasta_SAM/4483_SAM/Sample4_tuf.sam', './Files/multi_fasta_SAM/4483_SAM/Sample6_erm.sam', './Files/multi_fasta_SAM/4483_SAM/Sample6_blunt.sam', './Files/multi_fasta_SAM/4483_SAM/Sample5_tuf.sam', './Files/multi_fasta_SAM/4483_SAM/Sample6_pen.sam', './Files/multi_fasta_SAM/4483_SAM/Sample1_cap.sam', './Files/multi_fasta_SAM/4483_SAM/Sample4_dual.sam', './Files/multi_fasta_SAM/4483_SAM/Sample5_dual.sam', './Files/multi_fasta_SAM/4483_SAM/Sample1_blunt.sam', './Files/multi_fasta_SAM/4483_SAM/Sample3_cap.sam', './Files/multi_fasta_SAM/4483_SAM/Sample3_dual.sam', './Files/multi_fasta_SAM/4483_SAM/Sample2_dual.sam', './Files/multi_fasta_SAM/4483_SAM/Sample5_erm.sam', './Files/multi_fasta_SAM/4483_SAM/Sample4_pen.sam', './Files/multi_fasta_SAM/4483_SAM/Sample6_tuf.sam', './Files/multi_fasta_SAM/4483_SAM/Sample2_cap.sam', './Files/multi_fasta_SAM/4483_SAM/Sample5_pen.sam', './Files/multi_fasta_SAM/4483_SAM/Sample4_erm.sam', './Files/multi_fasta_SAM/3673_SAM/Sample2_blunt.sam', './Files/multi_fasta_SAM/3673_SAM/Sample6_dual.sam', './Files/multi_fasta_SAM/3673_SAM/Sample1_erm.sam', './Files/multi_fasta_SAM/3673_SAM/Sample3_tuf.sam', './Files/multi_fasta_SAM/3673_SAM/Sample6_cap.sam', './Files/multi_fasta_SAM/3673_SAM/Sample1_pen.sam', './Files/multi_fasta_SAM/3673_SAM/Sample2_tuf.sam', './Files/multi_fasta_SAM/3673_SAM/Sample4_blunt.sam', './Files/multi_fasta_SAM/3673_SAM/Sample3_blunt.sam', './Files/multi_fasta_SAM/3673_SAM/Sample2_erm.sam', './Files/multi_fasta_SAM/3673_SAM/Sample3_pen.sam', './Files/multi_fasta_SAM/3673_SAM/Sample4_cap.sam', './Files/multi_fasta_SAM/3673_SAM/Sample5_blunt.sam', './Files/multi_fasta_SAM/3673_SAM/Sample1_dual.sam', './Files/multi_fasta_SAM/3673_SAM/Sample2_pen.sam', './Files/multi_fasta_SAM/3673_SAM/Sample3_erm.sam', './Files/multi_fasta_SAM/3673_SAM/Sample5_cap.sam', './Files/multi_fasta_SAM/3673_SAM/Sample1_tuf.sam', './Files/multi_fasta_SAM/3673_SAM/Sample4_tuf.sam', './Files/multi_fasta_SAM/3673_SAM/Sample6_erm.sam', './Files/multi_fasta_SAM/3673_SAM/Sample6_blunt.sam', './Files/multi_fasta_SAM/3673_SAM/Sample5_tuf.sam', './Files/multi_fasta_SAM/3673_SAM/Sample6_pen.sam', './Files/multi_fasta_SAM/3673_SAM/Sample1_cap.sam', './Files/multi_fasta_SAM/3673_SAM/Sample4_dual.sam', './Files/multi_fasta_SAM/3673_SAM/Sample5_dual.sam', './Files/multi_fasta_SAM/3673_SAM/Sample1_blunt.sam', './Files/multi_fasta_SAM/3673_SAM/Sample3_cap.sam', './Files/multi_fasta_SAM/3673_SAM/Sample3_dual.sam', './Files/multi_fasta_SAM/3673_SAM/Sample2_dual.sam', './Files/multi_fasta_SAM/3673_SAM/Sample5_erm.sam', './Files/multi_fasta_SAM/3673_SAM/Sample4_pen.sam', './Files/multi_fasta_SAM/3673_SAM/Sample6_tuf.sam', './Files/multi_fasta_SAM/3673_SAM/Sample2_cap.sam', './Files/multi_fasta_SAM/3673_SAM/Sample5_pen.sam', './Files/multi_fasta_SAM/3673_SAM/Sample4_erm.sam', './Files/multi_fasta_SAM/3936_SAM/Sample2_blunt.sam', './Files/multi_fasta_SAM/3936_SAM/Sample6_dual.sam', './Files/multi_fasta_SAM/3936_SAM/Sample1_erm.sam', './Files/multi_fasta_SAM/3936_SAM/Sample3_tuf.sam', './Files/multi_fasta_SAM/3936_SAM/Sample6_cap.sam', './Files/multi_fasta_SAM/3936_SAM/Sample1_pen.sam', './Files/multi_fasta_SAM/3936_SAM/Sample2_tuf.sam', './Files/multi_fasta_SAM/3936_SAM/Sample4_blunt.sam', './Files/multi_fasta_SAM/3936_SAM/Sample3_blunt.sam', './Files/multi_fasta_SAM/3936_SAM/Sample2_erm.sam', './Files/multi_fasta_SAM/3936_SAM/Sample3_pen.sam', './Files/multi_fasta_SAM/3936_SAM/Sample4_cap.sam', './Files/multi_fasta_SAM/3936_SAM/Sample5_blunt.sam', './Files/multi_fasta_SAM/3936_SAM/Sample1_dual.sam', './Files/multi_fasta_SAM/3936_SAM/Sample2_pen.sam', './Files/multi_fasta_SAM/3936_SAM/Sample3_erm.sam', './Files/multi_fasta_SAM/3936_SAM/Sample5_cap.sam', './Files/multi_fasta_SAM/3936_SAM/Sample1_tuf.sam', './Files/multi_fasta_SAM/3936_SAM/Sample4_tuf.sam', './Files/multi_fasta_SAM/3936_SAM/Sample6_erm.sam', './Files/multi_fasta_SAM/3936_SAM/Sample6_blunt.sam', './Files/multi_fasta_SAM/3936_SAM/Sample5_tuf.sam', './Files/multi_fasta_SAM/3936_SAM/Sample6_pen.sam', './Files/multi_fasta_SAM/3936_SAM/Sample1_cap.sam', './Files/multi_fasta_SAM/3936_SAM/Sample4_dual.sam', './Files/multi_fasta_SAM/3936_SAM/Sample5_dual.sam', './Files/multi_fasta_SAM/3936_SAM/Sample1_blunt.sam', './Files/multi_fasta_SAM/3936_SAM/Sample3_cap.sam', './Files/multi_fasta_SAM/3936_SAM/Sample3_dual.sam', './Files/multi_fasta_SAM/3936_SAM/Sample2_dual.sam', './Files/multi_fasta_SAM/3936_SAM/Sample5_erm.sam', './Files/multi_fasta_SAM/3936_SAM/Sample4_pen.sam', './Files/multi_fasta_SAM/3936_SAM/Sample6_tuf.sam', './Files/multi_fasta_SAM/3936_SAM/Sample2_cap.sam', './Files/multi_fasta_SAM/3936_SAM/Sample5_pen.sam', './Files/multi_fasta_SAM/3936_SAM/Sample4_erm.sam', './Files/multi_fasta_SAM/3389_SAM/Sample2_blunt.sam', './Files/multi_fasta_SAM/3389_SAM/Sample6_dual.sam', './Files/multi_fasta_SAM/3389_SAM/Sample1_erm.sam', './Files/multi_fasta_SAM/3389_SAM/Sample3_tuf.sam', './Files/multi_fasta_SAM/3389_SAM/Sample6_cap.sam', './Files/multi_fasta_SAM/3389_SAM/Sample1_pen.sam', './Files/multi_fasta_SAM/3389_SAM/Sample2_tuf.sam', './Files/multi_fasta_SAM/3389_SAM/Sample4_blunt.sam', './Files/multi_fasta_SAM/3389_SAM/Sample3_blunt.sam', './Files/multi_fasta_SAM/3389_SAM/Sample2_erm.sam', './Files/multi_fasta_SAM/3389_SAM/Sample3_pen.sam', './Files/multi_fasta_SAM/3389_SAM/Sample4_cap.sam', './Files/multi_fasta_SAM/3389_SAM/Sample5_blunt.sam', './Files/multi_fasta_SAM/3389_SAM/Sample1_dual.sam', './Files/multi_fasta_SAM/3389_SAM/Sample2_pen.sam', './Files/multi_fasta_SAM/3389_SAM/Sample3_erm.sam', './Files/multi_fasta_SAM/3389_SAM/Sample5_cap.sam', './Files/multi_fasta_SAM/3389_SAM/Sample1_tuf.sam', './Files/multi_fasta_SAM/3389_SAM/Sample4_tuf.sam', './Files/multi_fasta_SAM/3389_SAM/Sample6_erm.sam', './Files/multi_fasta_SAM/3389_SAM/Sample6_blunt.sam', './Files/multi_fasta_SAM/3389_SAM/Sample5_tuf.sam', './Files/multi_fasta_SAM/3389_SAM/Sample6_pen.sam', './Files/multi_fasta_SAM/3389_SAM/Sample1_cap.sam', './Files/multi_fasta_SAM/3389_SAM/Sample4_dual.sam', './Files/multi_fasta_SAM/3389_SAM/Sample5_dual.sam', './Files/multi_fasta_SAM/3389_SAM/Sample1_blunt.sam', './Files/multi_fasta_SAM/3389_SAM/Sample3_cap.sam', './Files/multi_fasta_SAM/3389_SAM/Sample3_dual.sam', './Files/multi_fasta_SAM/3389_SAM/Sample2_dual.sam', './Files/multi_fasta_SAM/3389_SAM/Sample5_erm.sam', './Files/multi_fasta_SAM/3389_SAM/Sample4_pen.sam', './Files/multi_fasta_SAM/3389_SAM/Sample6_tuf.sam', './Files/multi_fasta_SAM/3389_SAM/Sample2_cap.sam', './Files/multi_fasta_SAM/3389_SAM/Sample5_pen.sam', './Files/multi_fasta_SAM/3389_SAM/Sample4_erm.sam', './Files/multi_fasta_SAM/4206_SAM/Sample2_blunt.sam', './Files/multi_fasta_SAM/4206_SAM/Sample6_dual.sam', './Files/multi_fasta_SAM/4206_SAM/Sample1_erm.sam', './Files/multi_fasta_SAM/4206_SAM/Sample3_tuf.sam', './Files/multi_fasta_SAM/4206_SAM/Sample6_cap.sam', './Files/multi_fasta_SAM/4206_SAM/Sample1_pen.sam', './Files/multi_fasta_SAM/4206_SAM/Sample2_tuf.sam', './Files/multi_fasta_SAM/4206_SAM/Sample4_blunt.sam', './Files/multi_fasta_SAM/4206_SAM/Sample3_blunt.sam', './Files/multi_fasta_SAM/4206_SAM/Sample2_erm.sam', './Files/multi_fasta_SAM/4206_SAM/Sample3_pen.sam', './Files/multi_fasta_SAM/4206_SAM/Sample4_cap.sam', './Files/multi_fasta_SAM/4206_SAM/Sample5_blunt.sam', './Files/multi_fasta_SAM/4206_SAM/Sample1_dual.sam', './Files/multi_fasta_SAM/4206_SAM/Sample2_pen.sam', './Files/multi_fasta_SAM/4206_SAM/Sample3_erm.sam', './Files/multi_fasta_SAM/4206_SAM/Sample5_cap.sam', './Files/multi_fasta_SAM/4206_SAM/Sample1_tuf.sam', './Files/multi_fasta_SAM/4206_SAM/Sample4_tuf.sam', './Files/multi_fasta_SAM/4206_SAM/Sample6_erm.sam', './Files/multi_fasta_SAM/4206_SAM/Sample6_blunt.sam', './Files/multi_fasta_SAM/4206_SAM/Sample5_tuf.sam', './Files/multi_fasta_SAM/4206_SAM/Sample6_pen.sam', './Files/multi_fasta_SAM/4206_SAM/Sample1_cap.sam', './Files/multi_fasta_SAM/4206_SAM/Sample4_dual.sam', './Files/multi_fasta_SAM/4206_SAM/Sample5_dual.sam', './Files/multi_fasta_SAM/4206_SAM/Sample1_blunt.sam', './Files/multi_fasta_SAM/4206_SAM/Sample3_cap.sam', './Files/multi_fasta_SAM/4206_SAM/Sample3_dual.sam', './Files/multi_fasta_SAM/4206_SAM/Sample2_dual.sam', './Files/multi_fasta_SAM/4206_SAM/Sample5_erm.sam', './Files/multi_fasta_SAM/4206_SAM/Sample4_pen.sam', './Files/multi_fasta_SAM/4206_SAM/Sample6_tuf.sam', './Files/multi_fasta_SAM/4206_SAM/Sample2_cap.sam', './Files/multi_fasta_SAM/4206_SAM/Sample5_pen.sam', './Files/multi_fasta_SAM/4206_SAM/Sample4_erm.sam']\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "sams = glob.glob(\"./Files/multi_fasta_SAM/*/*.sam\")\n",
    "#sams = glob.glob('*.py')\n",
    "print(sams)\n",
    "print(len(sams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4483 SAM files - DOWNLOAD US .zip',\n",
       " '4483_SAM',\n",
       " '3673_SAM',\n",
       " '.DS_Store',\n",
       " '3936_SAM',\n",
       " '3673 SAM files - DOWNLOAD US .zip',\n",
       " '3389 SAM files - DOWNLOAD US .zip',\n",
       " '3936 SAM files - DOWNLOAD US .zip',\n",
       " '4206 SAM files - DOWNLOAD US .zip',\n",
       " '3389_SAM',\n",
       " '4206_SAM']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "os.listdir(\"./Files/multi_fasta_SAM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tailob/Desktop/jax/repositories/tnseq-meta-integration'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the script you should get a tabular file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><h1>Monday April 29 - </h1></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #40E0D0;\">GFF files</span>\n",
    "\n",
    "1. Login Galaxy\n",
    "2. Load the fasta files \n",
    "3. Search for <a ref = \"https://www.youtube.com/watch?v=54ECEqJwYz4\">Prokka (Prokaryotic Genome annotation)</a> on the right hand side.\n",
    "4. Under the section 'Contigs to annotate' upload fasta file or reference files already uploaded\n",
    "    - NC_007795.1 (parent strain)\n",
    "4. Run the tool\n",
    "5. Output should appear in histories in about 15 minutes\n",
    "6. next step is to <a ref = \"https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000013425.1/\">look up</a> the other strains\n",
    "    - NCTC 8325 (NC_007795.1—HG003 parent strain) \n",
    "    - USA300-TCH1516 (NC_010079.1)\n",
    "    - MSSA476 (NC_002953.3)\n",
    "    - MW2 (NC_003929.1) //  <span style=\"color: red;\">since the original was not available I used NC_003923.1\n",
    "    - MRSA252 (NC_002952.2) </a>\n",
    "7. Click on FTP and find the gff table\n",
    "8. Make a prot table with both Prokka and pubically available gff files..    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prot table code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Goal: Take gff output from Prokka and make .prot_table required input for TRANSIT Gumbel. \n",
    "\n",
    "# # Packages\n",
    "import glob\n",
    "\n",
    "# # Function\n",
    "def make_prot_table(gff):\n",
    "\tinput = open(gff,'r')\n",
    "\toutput = open(gff[:-3]+\"prot_table\",'w')\n",
    "\tfor line in input.readlines(): \n",
    "\t\tif line.startswith(\"##FASTA\"):\n",
    "\t\t\tbreak\n",
    "\t\telif not line.startswith(\"#\"):\n",
    "\t\t\tinfo = line.split(\"\\t\")\n",
    "\t\t\tif info[2] != 'repeat_region':\n",
    "\t\t\t\tstart = info[3]\n",
    "\t\t\t\tstop = info[4]\n",
    "\t\t\t\tstrand = info[6]\n",
    "\t\t\t\tdetails = info[-1].split(';')\n",
    "\t\t\t\tdetdict = {}\n",
    "\t\t\t\tfor d in details: \n",
    "\t\t\t\t\tdetdict[d.split(\"=\")[0]]=d.split(\"=\")[1]\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\ttag = detdict[\"ID\"]\n",
    "\t\t\t\texcept: print(line)\n",
    "\t\t\t\tif \"Name\" in detdict: \n",
    "\t\t\t\t\tgene = detdict[\"Name\"]\n",
    "\t\t\t\telse: gene = '-'\n",
    "\t\t\t\tprod = detdict[\"product\"].rstrip()\n",
    "\t\t\t\tprodlen = int((int(stop)-int(start))/3)\n",
    "\t\t\t\tfor thing in [prod,start,stop,strand,prodlen,0,0,gene,tag]:\n",
    "\t\t\t\t\toutput.write(str(thing)+'\\t')\n",
    "\t\t\t\toutput.write('-\\n')\n",
    "# # RUN\n",
    "if __name__==\"__main__\": \n",
    "\tgffs = glob.glob(\"~/Dropbox/Breeya_rotation/Files/gff_files/*.gff3\")\n",
    "\tfor g in gffs: \n",
    "\t\tprint(g)\n",
    "\t\tmake_prot_table(g)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gffs = glob.glob(\"./Files/gff_files/*.gff3\")\n",
    "print((gffs))\n",
    "for g in gffs: \n",
    "    print(g)\n",
    "    make_prot_table(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tailob/Desktop/jax/repositories/tnseq-meta-integration\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print (cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create TA site files (runs on all fastas in directory) `python findTASites.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Make a list of the TA sites in a nucleotide fasta file\n",
    "\n",
    "import glob \n",
    "\n",
    "def find_tas(fasta):\n",
    "\tfile = open(fasta,'r')\n",
    "\tout=open(fasta[:-6]+'_TASites.txt','w')\n",
    "\tout.write(\";PatID\\tStrand\\tPattern\\tSeqID\\tStart\\tEnd\\tmatching_seq\\tScore\\n\")\n",
    "\tct = 0\n",
    "\tlooking = False\n",
    "\tfor line in file.readlines():\n",
    "\t\tif line.startswith('>'):\n",
    "\t\t\tacc = line.split(' ')[0][1:]\n",
    "\t\telse: \n",
    "\t\t\tfor nt in line:\n",
    "\t\t\t\tif nt != '\\n':\n",
    "\t\t\t\t\tct+=1\n",
    "\t\t\t\t\tif nt == 'T':\n",
    "\t\t\t\t\t\tlooking = True\n",
    "\t\t\t\t\telif looking and nt != 'A':\n",
    "\t\t\t\t\t\tlooking = False\n",
    "\t\t\t\t\telif looking and nt == 'A':\n",
    "\t\t\t\t\t\tlooking = False\n",
    "\t\t\t\t\t\tout.write('\\t'.join(['TA','D','TA',acc,str(ct-1),str(ct),'TA','1'])+'\\n')\n",
    "\t\t\t\t\n",
    "# # RUN\n",
    "if __name__==\"__main__\": \n",
    "\tfastas = glob.glob(\"/Users/tailob/Desktop/jax/repositories/tnseq-meta-integration/Files/fasta_files/*.fasta\")\n",
    "\tfor f in fastas: \n",
    "\t\tprint(f)\n",
    "\t\tfind_tas(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "./Files/fasta_files/NC_010079.1_USA300-TCH1516_sequence.fasta\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_TASites' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m gffs: \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(g)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43m_TASites\u001b[49m\u001b[38;5;241m.\u001b[39mtxt(g)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_TASites' is not defined"
     ]
    }
   ],
   "source": [
    "gffs = glob.glob(\"./Files/fasta_files/*.fasta\")\n",
    "print((fastas))\n",
    "for g in gffs: \n",
    "    print(g)\n",
    "    _TASites.txt(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.  __make_wig.py__ - Makes a wig file recognized by the TRANSIT software from a tabular file and a list of TA sites created using findTASites.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Convert tabular files to wig files that can be used with the TRANSIT essential gene identification platform. \n",
    "# To run: python make_wigs.py TAList.txt data.tabular\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "def make_TAdict(TAfile):\n",
    "\tTAs = {}\n",
    "\tfor line in open(TAfile,'r').readlines():\n",
    "\t\tif not line.startswith(';'):\n",
    "\t\t\tinfo = line.split('\\t')\n",
    "\t\t\tpos = int(info[4])\n",
    "\t\t\tTAs[pos]=0\n",
    "\treturn TAs\n",
    "\n",
    "def tab_to_wig(tabular,TAdict):\n",
    "\tout = open(tabular[:-7]+'wig', 'w')\n",
    "\tout.write(\"variableStep  chrom=chr1\\n\")\n",
    "\tfile = open(tabular,'r')\n",
    "\tfor line in file: \n",
    "\t\tinfo = line.split(\"\\t\")\n",
    "\t\tpos = 0 \n",
    "\t# # Make sure you're looking at a line with real data\n",
    "\t\tif len(info)>= 6 and info[1].isdigit():\n",
    "\t\t# # Look for reads in the positive direction and change site to TA site instead of sequencing start site. \n",
    "\t\t\tif int(info[4]) != 0:\n",
    "\t\t\t\tpos = int(info[1])\n",
    "\t\t\t\tif pos+15 in TAdict:\n",
    "\t\t\t\t\tTAdict[pos+15] += int(info[4])\n",
    "\t\t\t\telif pos+14 in TAdict:\n",
    "\t\t\t\t\tTAdict[pos+14] += int(info[4])\n",
    "\t\t##Do the same with the reads that are in the reverse direction and change site to TA site instead of sequencing start site.  \n",
    "\t\t\tif int(info[5]) != 0:\n",
    "\t\t\t\tpos = int(info[1])\n",
    "\t\t\t\tif pos-15 in TAdict: \n",
    "\t\t\t\t\tTAdict[pos-15] += int(info[5])\n",
    "\t\t\t\telif pos-16 in TAdict:\n",
    "\t\t\t\t\tTAdict[pos-16] += int(info[5])\n",
    "\tfile.close()\n",
    "\t\n",
    "\tkeys = list(TAdict.keys())\n",
    "\tkeys.sort()\n",
    "\n",
    "\t## Everything gets written to the output IGV file.  \n",
    "\tfor k in keys:\n",
    "\t\tout.write('%d\\t%d\\n' %(k,TAdict[k]))\n",
    "\t\n",
    "\tout.close()\n",
    "\treturn\n",
    "\t\n",
    "#RUN\n",
    "#-------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Use glob to find all files that match the patterns\n",
    "    ta_files = glob.glob(\"/Users/tailob/Desktop/jax/repositories/tnseq-meta-integration/Files/TAsite_files/*.txt\")\n",
    "    tabular_files = glob.glob(\"/Users/tailob/Desktop/jax/repositories/tnseq-meta-integration/Files/tabular_files/*.tabular\")\n",
    "\n",
    "    # Iterate over each TA site file\n",
    "    for ta_file in ta_files:\n",
    "        TAs = make_TAdict(ta_file)\n",
    "        # Iterate over each tabular file\n",
    "        for tabular_file in tabular_files:\n",
    "            tab_to_wig(tabular_file, TAs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Use glob to find all files that match the patterns\n",
    "    ta_files = glob.glob(\"/Users/tailob/Desktop/jax/repositories/tnseq-meta-integration/Files/TAsite_files/*.txt\")\n",
    "    tabular_files = glob.glob(\"/Users/tailob/Desktop/jax/repositories/tnseq-meta-integration/Files/tabular_files/*/*.tabular\")\n",
    "\n",
    "    # Iterate over each TA site file\n",
    "    for ta_file in ta_files:\n",
    "        TAs = make_TAdict(ta_file)\n",
    "        # Iterate over each tabular file\n",
    "        for tabular_file in tabular_files:\n",
    "            print(tabular_file)\n",
    "            tab_to_wig(tabular_file, TAs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Wig files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTs = ['Files/Prot_tables/NC_002952.2_MRSA252_Galaxy4556.gprot_table', 'Files/Prot_tables/NC_002953.3_MSSA476_Galaxy4544.gprot_table', 'Files/Prot_tables/NC_010079.1_USA300-TCH1516_Galaxy4532.gprot_table', 'Files/Prot_tables/NC_003923.1_MW2_Galaxy4568.gprot_table', 'Files/Prot_tables/NC_007795.1_parent strain_Galaxy4520.gprot_table']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Files/Prot_tables/NC_002952ProkkaTag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Files/Prot_tables/NC_002952ProkkaTag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m pt\u001b[38;5;241m.\u001b[39mitertuples():\n\u001b[1;32m     30\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m row\u001b[38;5;241m.\u001b[39mTag \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m \t\troarytag \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mName[\u001b[43mtags\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstrain\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProkkaTag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39mrow\u001b[38;5;241m.\u001b[39mTag]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m \t\tpt\u001b[38;5;241m.\u001b[39mloc[row\u001b[38;5;241m.\u001b[39mIndex,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGene\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mroarytag\n\u001b[1;32m     33\u001b[0m pt\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFiles/Prot_tables/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m prottable, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Files/Prot_tables/NC_002952ProkkaTag'"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import glob \n",
    "\n",
    "# Functions: \n",
    "# Read in a TRANSIT-style prot_table file. \n",
    "def read_prot_table(filename):\n",
    "\tprottable = pd.read_csv(filename,sep='\\t',header=None,names=['Description','Start','Stop','Strand','ProtLength','col1','col2','Gene','Tag','col3'])\n",
    "\treturn prottable\n",
    "\t\n",
    "# Read in a TRANSIT-style wig file. \n",
    "def read_wig(filename):\n",
    "\twig = pd.read_csv(filename,sep='\\t',header=0,names=['TA','Reads'])\n",
    "\treturn wig \n",
    "\n",
    "# Executable:\n",
    "# Open the list of corresponding tags\n",
    "tags = pd.read_csv('MasterTagList.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Collect the names of the prot_table files \n",
    "PTs = glob.glob('Files/Prot_tables/*.gprot_table')\n",
    "\n",
    "print(\"PTs = {}\".format(PTs))\n",
    "\n",
    "# Loop through the prot_table files and change the gene name to the Roary tag\n",
    "for prottable in PTs: \n",
    "\tstrain = prottable.split('.')[0]\n",
    "\tpt = read_prot_table(prottable)\n",
    "\tfor row in pt.itertuples():\n",
    "\t\tif row.Tag != '-':\n",
    "\t\t\troarytag = tags.Name[tags[strain+'ProkkaTag']==row.Tag].values[0]\n",
    "\t\t\tpt.loc[row.Index,'Gene']=roarytag\n",
    "\tpt.to_csv('Files/Prot_tables/' + prottable, sep='\\t', header=False, index=False)\n",
    "\n",
    "# Gather the names of the wig files\n",
    "wigs = glob.glob('Files/wig_files/3389_NC_010079.1_USA300-TCH1516_wig/*.wig')\n",
    "\n",
    "strains = list(set([x.split('_')[0] for x in wigs]))\n",
    "\n",
    "\n",
    "print(\"wigs = {}\".format(wigs))\n",
    "\n",
    "# For each wig file, label the TA sites with the gene they are found in, using the Roary tags\n",
    "# We do this strain-wise because the labeling takes a while, but it should be the same for every file for the strain, so we can save time by just reusing the results for the first file in a strain. \n",
    "for strain in strains:\n",
    "\tgenes = ''\n",
    "\tfor w in wigs: \n",
    "\t\tif strain in w:\n",
    "\t\t\twig = read_wig(w)\n",
    "\t\t\tif type(genes)==str:\n",
    "\t\t\t\twig['Gene']=''\n",
    "\t\t\t\tstrain = w.split('_')[0]\n",
    "\t\t\t\tprot = read_prot_table('Roary_'+strain+'.prot_table')\n",
    "\t\t\t\tfor gene in prot.itertuples():\n",
    "\t\t\t\t\twig.loc[wig.index[(wig.TA>gene.Start) & (wig.TA<gene.Stop)],'Gene']=gene.Gene\n",
    "\t\t\t\t\n",
    "\t\t\t\tgenes = wig.Gene\n",
    "\t\t\telse: wig['Gene']=genes\n",
    "\t\t\twig.to_csv('Files/wig_files/*/'+w[:-3]+'csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wigs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mwigs\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wigs' is not defined"
     ]
    }
   ],
   "source": [
    "print(wigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Files/Prot_tables/NC_002952.2_MRSA252_Galaxy4556.gprot_table', 'Files/Prot_tables/NC_002953.3_MSSA476_Galaxy4544.gprot_table', 'Files/Prot_tables/NC_010079.1_USA300-TCH1516_Galaxy4532.gprot_table', 'Files/Prot_tables/NC_003923.1_MW2_Galaxy4568.gprot_table', 'Files/Prot_tables/NC_007795.1_parent strain_Galaxy4520.gprot_table']\n"
     ]
    }
   ],
   "source": [
    "print(PTs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><h1>TRANSIT2</h1></p>\n",
    "\n",
    "Transit2 is a python-based software system that combines statistical algorithms for analyzing TnSeq data (from sequencing transposon-mutant libraries). It has GUI (graphical-user interface) to make it easy for users to process their data, conduct essentiality analyses, and visualize results. Transit routines can also be run from the command line. It works in Linux, MacOS, and Windows. Transit requires various python packages to be installed, as well as R (a statistical program that Transit calls for some analyses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color: #40E0D0;\">TRASIT2 has two main phases</span></h2>\n",
    "\n",
    "<h3> 1. A pre-processing phase (called TPP)</h3><br>\n",
    "    Analyzes raw sequencing data (fastq files) and extracts transposon insertion counts at genomic coordinates (i.e. TA dinucleotides, for the Himar1 transposon), and\n",
    "\n",
    "<h3> 2. Statistical analysis methods</h3>\n",
    "    <br>\n",
    "    (using Transit GUI or command line) - gumbel, HMM, resampling, ANOVA. The statistical analyses are focused on identifying genes that are essential or conditionally-essential, and quantifying the statistical significance of these.    \n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"color: #FFC4CA;\">\n",
    "Understanding the process\n",
    "<br>\n",
    "\n",
    "1. Collection of Data:<br>\n",
    "        Imagine you have a bacterial genome with 1000 genes.\n",
    "        You perform TnSeq and get counts of transposon insertions for each gene.\n",
    "\n",
    "2. Observation:<br>\n",
    "        Most genes have around 10-20 insertions.\n",
    "        A few genes have very high or very low insertion counts.\n",
    "3. Application of Gumbel Method:<br>\n",
    "        You use the Gumbel method to model the distribution of insertion counts.\n",
    "        This helps identify insertion counts that are extreme (much higher or lower than typical).\n",
    "\n",
    "4. Identifying Significance:<br>\n",
    "        Genes with significantly fewer insertions are likely essential (because disrupting them is lethal).\n",
    "        Genes with many insertions are likely non-essential. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/tailob/Desktop/jax/repositories/tnseq-meta-integration\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Essential Gene Analysis\n",
    "\n",
    "We identified the essential genes using the TRANSIT software Gumbel method (1). The scripts below are those used to convert files so that they could be used with the TRANSIT software and to perform a permutation test to determine which genes have significant fitness differences across strains of *S. aureus*. \n",
    "\n",
    "### Files Needed\n",
    "- __*.tabular__ - The tabular files acquired from processing the FASTQ files (see above).\n",
    "- __*.fasta__ - A chromosome nucleotide fasta for the genome you are mapping to. See paper methods for the NCBI accession numbers for the strains used.\n",
    "- __*.gff__ - A GFF file output from running Prokka (2) on the fasta files.\n",
    "- __make_prot_tables.py__ - A python script that converts the GFF files from Prokka into prot_table files for TRANSIT. \n",
    "- __findTASites.py__ - For each fasta file in the directory, makes a list of all of the TA sites available.\n",
    "- __make_wig.py__ - Makes a wig file recognized by the TRANSIT software from a tabular file and a list of TA sites created using findTASites.py.\n",
    "- __labelWIGs.py__ - Using the master tag list, changes the gene names in the prot_table files into Roary tags that can be compared across all strains. Then it uses these new prot_tables to annotate the genes in the wig files. For this script to work, the prot_tables have to be named just with the name of the strain and the wig filename has to also include the strain. This script works on all prot_tables and wigs in the directory. The annotated WIGs are saved as csv files.  \n",
    "- __MasterTagList.csv__ - A list of gene names and locations from different strains matched up using Roary (3). NCBI tags are also included, matched to the Prokka genes based on location. \n",
    "- __permutation.py__ - Adds together the reads from all annotated wig files for a strain (file name must start with Roary_*Strain* and must be a csv) and then for each pair of strains, performs a permutation test.  \n",
    "\n",
    "### Process\n",
    "1. Create Prot_Tables (runs on all GFFs in directory) `python make_prot_tables.py`\n",
    "2. Create TA site files (runs on all fastas in directory) `python findTASites.py`\n",
    "3. Create wig files (runs on a single fasta-tabular file pair) `python make_wig.py <XXX_TASites.txt> <XXX.tabular>`\n",
    "4. Run TRANSIT (see <https://pythonhosted.org/tnseq-transit/index.html> for more information).\n",
    "5. Make sure names are compatible with labelWIGs.py and permutation.py (see file descriptions above). \n",
    "6. Rename the genes in the Prot_Tables based on the Roary tags. `python labelWIGs.py` \n",
    "7. Perform the permutation test. By comparing the outputs of this script to the TRANSIT outputs, you can determine which differences in gene essentiality identified by TRANSIT are significant.  `python permutation.py`\n",
    "\n",
    "## Identifying Depleted/Enriched/Upregulated Genes in Treated Files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
